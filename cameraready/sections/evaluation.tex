\section{User Evaluation}
\label{sec:evaluation}

To improve the current implementation and to assess the feasibility of the developed Speech2RuM approach based on user feedback, we conducted a qualitative user study. Our aim was to (1) identify means for improving the interface, (2) identify issues and demands of individuals with different backgrounds, and (3) identify usage scenarios. In the following, we will describe the study before discussing our findings and potential threats to validity.

% remove expectation

\subsection{Study}
\label{sec:evaluation:setup}
\mypar{Participants} For our study, we selected eight participants (P1 to P8) with differing characteristics. We selected two participants that had experience related to Business Process Management  (BPM, tier 1, P6 and P7), two participants with formal background on temporal logics (tier 2, P3 and P8), two participants that had used \Declare\ to model temporal constraints (tier 3, P2 and P4), and two participants that had used different tools to model and analyze temporal constraints with \Declare\ (tier 4, P1 and P5). We chose this differentiation to study how individuals with different levels of expertise perceive the tool and identify potential issues and demands.

\mypar{Study setup}
The study was conducted via Skype by a team consisting of a \emph{facilitator} and an \emph{observer}, with the facilitator guiding the participant and the observer serving in a supporting role. We used a manufacturing process in a bicycle factory\footnote{The scenario can be found at \url{https://git.io/JfHbl}} to provide a scenario that is likely to be familiar for all participants. We opted to use a common scenario rather than asking participants to describe a process of their choice to ensure the comparability of our findings.
%, because every participant could be expected to be somehow familiar with such a process.
We also created a help document\footnote{The help document can be found at \url{https://git.io/JfHbc}} to outline the tool's capabilities to the participants.
%explains to the participants the different constraints that the tool could handle.

Prior to the study, the facilitator sent the participants a link to the scenario, the help document, and the tool itself suggesting to read the documents and install the tool. At the beginning of the study, the facilitator introduced the study procedure and the tool.
% by showing how to enter a single constraint using vocal input.
%He also asked the participant to think aloud during the study.
When ready, the participant shared her/his screen, whereas the facilitator started the video recording and began guiding the participant through the prepared scenario.
The scenario required the participants to construct a declarative process model. First, they added constraints by reading prepared sentences, before being asked to establish their own constraint descriptions.
%Initially, participants added constraints by reading prepared sentences, while afterwards
% starting with reading prepared sentences representing constraints related to the scenario
%and then creating constraints without prior guidance.
During this time, the facilitator and the observer noted down any issues the participant mentioned or that appeared to come up.% according to the usability heuristics by Nielsen~\cite{nielsen1994enhancing}.

At the end of the study, the facilitator conducted a follow-up interview, focusing on clarifying questions about issues the participant had encountered. He also asked the participants
what s/he \textit{``liked about the tool''}, \textit{``wished would be different''}, and \textit{``under which circumstances s/he would use it''}. After the study, the participants were asked to complete a short post-questionnaire. The questionnaires consisted of multi-point Likert scales including the System Usability Scale (SUS) \cite{brooke1996sus} and scales covering satisfaction, expectation confirmation, continuation intention, and usefulness which were adapted from the ones presented by Bhattacherjee in \cite{bhattacherjee2001understanding} and used to assess continued use of information systems.\footnote{The complete questionnaire can be found at \url{https://git.io/JfHb8}}
The individual studies lasted between 14 and 35 minutes each.
% about what s/he \textit{``liked about the tool''}, what s/he \textit{``wished would be different''} and \textit{``under which circumstances s/he would use the tool''}.

%\begin{table}[t!]
%	\caption{Questionnaire results represented as means across the different groups}
%	\label{tab:questionnaire}
%	\begin{tabular}{lccccc}
		
%		 & \textbf{all} & \textbf{tier 1} & \textbf{tier 2} & \textbf{tier 3} & \textbf{tier 4 }\\
%&  & \textbf{(P6, P7)} & \textbf{(P3, P8)} & \textbf{(P2, P4)} & \textbf{(P1, P5)}\\
%		\midrule
%        \textbf{SUS} & 73.13 & 71.25 & 81.25 & 67.50 & 72.50\\
%        \textbf{Satisfaction} & 3.46 & 3.67 & 3.67 & 3.00 & 3.50\\
%        \textbf{Expectation} & 3.69 & 3.67 & 3.50 & 3.50 & 4.00\\
%        \textbf{Future intentions} & 2.61 & 2.33 & 2.00 & 2.00 & 3.67\\
%        \textbf{Usefulness} & 3.00 & 2.63 & 3.13 & 2.75 & 3.50\\
%		\bottomrule
%	\end{tabular}
%\end{table}

\mypar{Result analysis}
To analyze the collected data, the research team first built an affinity diagram~\cite{holtzblatt2004rapid} based on the video recordings, observations, and follow-up interviews, by creating a single paper note for each issue that was reported. The team then clustered the notes based on emerging themes, which resulted in 139 notes divided over 21 distinct clusters. The results of the questionnaires served as an additional qualitative data point during the analysis.

\subsection{Findings}
\label{sec:evaluation:findings}
In general, our study revealed that the tested interface was reasonably usable as evidenced by an average SUS score of 73.13, which can be considered to be a good result compared to the commonly reported average of 68 \cite{brooke1996sus}. However, our study also revealed usability issues, which served as a basis to improve the tool (\figurename~\ref{fig:toolNew}). The main changes include the possibility to undo changes to the model based on the last recording, editing the recorded text directly rather than having to re-record it, and adding data conditions through speech recognition (instead of specifying those conditions manually).
%We also removed the previously included ``cancel'' button because it was often pressed by participants since they thought it would just stop the recording, whereas that button was supposed to discard the current recording.

\mypar{Common observations}
First, we found that all participants \textbf{wrote sentences down} before saying them aloud. They either wrote them \textit{``on paper''} (P1), \textit{``typed them on [their] computer''} (P2), or noted down key parts they wanted to say (P7). Most participants also used somewhat \textbf{unnatural sentences} when entering constraints. This is evidenced by them, e.g., using activity names instead of natural sentences (\textit{``after [activity1], [activity2] is executed.''}, P7). Some even tried to identify keywords that the algorithm might pick up (\textit{``is activity like a keyword?''}, P8). Only P1 used natural sentences from the start. Taking these findings together indicates that users might have felt insecure to just use natural sentences and rather tried to create sentences that they felt the system would be able to understand, but that were difficult for them to formulate without writing them down first.
% the fact that the context was somehow ``artificial'' in the form of a  predefined scenario.

%belonging to domains the participants were familiar with, without any explicit suggestions, could make the input formulation more natural.

Trying to identify keywords and adapting the input statements
for the tool might have \textbf{increased the complexity of entering constraints} while being counterproductive when using speech recognition since our tool was designed to understand natural expressions.
These issues indicate that \textbf{individuals need to learn how to effectively use speech as a means of modeling constraints} regardless of their previous experience related to BPM or \Declare. Therefore, it would be desirable for users to go through a training phase to better understand how to interact with the tool and also appreciate the variety of vocal inputs the tool can deal with.

\mypar{Difference between participants with different backgrounds}
There were also noticeable differences between individuals from different backgrounds. For example, participants from tiers 1 and 2 (having low confidence with \Declare) \textbf{mainly relied on the visual process model} to assess whether the tool had understood them correctly, while participants from tiers 3 and 4 (more familiar with \Declare\ and analysis tools based on \Declare) \textbf{mainly relied on the list providing a semistructured representation} of the entered constraints (bottom-right area in \figurename~\ref{fig:toolNew}). This was evidenced by how they tried to fix potential errors. Participants from tiers 1 and 2 initially tried to edit the visual process model (observation of P6, \textit{``the activity label should be editable''}, P2) -- which was not possible in the version of the tool we tested -- while participants from tiers 3 and 4 directly started editing the constraint list (observation of P1 and P5). Only P2 attempted both. %Given the different levels of confidence of the participants with \Declare, it appears reasonable that each group would mainly focus on the visualization that they are most familiar with.

%Taking the aforementioned findings together it appears as if creating spoken constraints that would be turned into formal constraints might have \textbf{increased the complexity of entering constraints} in particular for less experienced participants. Trying to identify keywords requires for users to formalize constraints in a way that might even be counterproductive when using speech input since our tool was created with the aim of understanding natural sentences. Moreover, it appears difficult for less experienced uses to assess the correctness of their entered constraints. This might be partly due to their potential lack of understanding \Declare. One approach to mitigate this issue might be to provide the option to mark constraints that a user cannot verify and subsequently ask an experienced user to assess the correctness.

%Trying to identify keywords and adapt the input statements to allow the tool to recognize the vocal input might have \textbf{increased the complexity of entering constraints} in particular for less experienced participants, since it requires users to formalize constraints in a way that might even be counterproductive when using speech input since our tool was created with the aim of understanding natural sentences. Moreover, it appears difficult for less experienced uses to assess the correctness of their entered constraints. This might be partly due to their potential lack of understanding \Declare. One approach to mitigate this issue might be to provide the option to mark constraints that a user cannot verify and subsequently ask an experienced user to assess the correctness.

Findings from our study also indicated that the way the \textbf{recognized text} was displayed (text field below \textit{Voice input} in the top-left area of the screenshot in \figurename~\ref{fig:toolNew}) might not have been ideal. Participants checked the input correctness after they entered the first constraint, but did not continue to do so afterward (observation of P3 and P6) despite few sentences being wrongly translated from speech into text during the tests. %Translation accuracy was especially problematic for participants who were not native English speakers and had a heavy accent.
%only referred to it after they had entered the first constraint and then did not check it again for future constraints (observation of P3 and P6).
This led to confusion, particularly among less experienced participants, who thought that they did not formulate a constraint correctly, while the issue was that their vocal input was not recorded properly.
One way to address this issue is to highlight the text field containing the recognized text directly after a new text has been recorded to indicate to the user that s/he should pay attention to the recognized text.

These findings indicate that, even if using speech recognition users can somehow bypass the interaction with the graphical notation of \Declare\ constraints, \textbf{it still appears difficult for less experienced users to assess the correctness of their entered constraints}. One approach to mitigate this issue might be to provide the option to mark constraints that a user cannot verify and subsequently ask an experienced user to assess their correctness. In alternative, traces representing examples and counterexamples of the recorded behaviors together with a textual description of the constraint could be shown.

The aforementioned issues might also have led to \textbf{less experienced participants perceiving the tool as less useful} (m = 2.63 for tier 1 and m = 3.50 for tier 4) and being \textbf{less inclined to use the tool in the future} (m = 2.33 for tier 1 and m = 3.65 for tier 4) than experienced participants. Related to this finding, it thus appears counter-intuitive that experienced participants were slightly less satisfied with the tool than less experienced participants (m = 3.00 for tier 3, m = 3.50 for tier 4, and m = 3.67 for tiers 1 and 2). This can however potentially be explained by experienced participants having higher expectations regarding the functionality of the tool (\textit{``what about recording data conditions as well?''}, P5).

Finally, it should be noted that participants from different backgrounds were \textit{``excited''} (P5) to use the tool. Some even continued to record constraints after the actual evaluation was over (\textit{``let me see what happens when I [...]''}, P8). It thus appears reasonable to assume that addressing the identified shortcomings can positively influence the perception of the tool.

\mypar{Potential usage scenarios}
The participants mentioned multiple scenarios during which they would consider speech as useful for entering constraints. Most of them stated that using speech would \textbf{improve their efficiency} because they could \textit{``quickly input stuff''} (P4) especially when reading \textit{``from a textual description''} (P1) with P5 pointing out that it would be particularly useful for \textit{``people that are not familiar with the editor''} (P5). On the other hand, P8 had a different opinion stating that \textit{``manually is quicker''} (P8). S/he did however acknowledge that speech might be useful \textit{``when I forget the constraint name''} (P8).
%thus indicating that \textbf{using natural language sentences} might be useful even when not using speech recognition.
Moreover, participants also suggested that vocal input might be useful \textit{``for designing''} (P3) which points towards its usefulness for specific activities at the early stages of modeling a process.
%
Finally, P1 mentioned that speech input would be useful for \textbf{mobile scenarios} such as \textit{``using a tablet''} (P1), because entering text in a scenario like that is usually much more time consuming than when using a mechanical keyboard.

\subsection{Threats to validity}
\label{sec:evaluation:limitations}
The goal of our study was to collect feedback for improving the tool, identify potential usage scenarios, and pinpoint any issues for individuals with different backgrounds. It thus appeared reasonable to conduct an in-depth qualitative study with selected participants from a diverse range of backgrounds related to their knowledge and experience with BPM in general and temporal properties and \Declare\ in particular. Conducting a study with a small sample of participants is common because research has shown that the number of additional insights gained deteriorates drastically per participant \cite{nielsen1993mathematical}. There are, however, some threats to validity associated with this particular study design. A first threat is related to the fact that we developed a specific tool and studied its use by specific people in a specific setting over a limited period of time. Despite carefully selecting participants and creating a setting that would be close to how we envision the tool would be commonly used, it is not possible to generalize our findings beyond our study context since conducting the same study with different participants using a different setup might yield different results. In addition, the study results were synthesized by a specific team of researchers which poses a threat to validity since different researchers might potentially interpret findings differently. We attempted to mitigate this threat by ensuring that observations, interviews, and the analysis of the obtained data were collaboratively conducted by two researchers. We also abstained from making causal claims providing instead a rich description of the behavior and reporting perceptions of participants. 