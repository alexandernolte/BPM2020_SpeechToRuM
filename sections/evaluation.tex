\section{User Evaluation}
\label{sec:evaluation}

In order to improve the current implementation and assess the overall feasibility of the developed Speech2RuM approach, we conducted a qualitative case study. The threefold aim was to (1) identify means for improving the interface, (2) identify potential issues and demands of individuals with different backgrounds, and (3) identify potential usage scenarios.
In the following, we will first describe the study setup including the data we collected and our analysis procedure before discussing our findings and limitations of the study setup.

\subsection{Study setup}
\label{sec:evaluation:setup}
\mypar{Participants} For our study, we selected a total of eight participants with differing characteristics (referred to as P1 to P8 in \tablename~\ref{tab:questionnaire}).
In particular, we selected two participants that only had experience related to Business Process Management  (BPM, tier 1), two participants with formal background on temporal logics (tier 2), two participants that had experience related to using \Declare\ for modeling temporal constraints (tier 3), and two participants that had experience related to using \Declare\ and a variety of tools for modeling and analyzing temporal constraints (tier 4). We chose this differentiation to gain insight about the usability of the tool for individuals with different levels of expertise and to assess potential issues and demands by individuals with different backgrounds.

\mypar{Study setup}
The study was conducted via Skype by a team consisting of a \emph{facilitator} and an \emph{observer}, with the facilitator guiding the participant and the observer serving in a supporting role. We used the manufacturing process in a bicycle factory\footnote{The scenario can be found at \url{https://git.io/Jv1Hn}} to provide a familiar contextual scenario to the study. 
%, because every participant could be expected to be somehow familiar with such a process. 
We also created a help document\footnote{The help document can be found at \url{https://git.io/Jv1Hc}} to outline the tool's capabilities to the participants. 
%explains to the participants the different constraints that the tool could handle.

Prior to the study, the facilitator sent the participants a link to the scenario, the help document, and the tool itself. He suggested them to read the documents and install the tool. At the beginning of the study, the facilitator introduced the participant to the study procedure and the tool.
% by showing how to enter a single constraint using vocal input. 
%He also asked the participant to think aloud during the study. 
When ready, the participant shared her/his screen, whereas the facilitator started the video recording and began guiding the participant through the prepared scenario.
In this scenario, the participants were asked to construct a declarative process model. First, they added constraints by reading prepared sentences, before being asked to establish their own constraint descriptions.
%Initially, participants added constraints by reading prepared sentences, while afterwards 
% starting with reading prepared sentences representing constraints related to the scenario
%and then creating constraints without prior guidance. 
During this time, the facilitator and observer noted down any issues the participant mentioned or where they appeared to arise according to the usability heuristics by Nielsen~\cite{nielsen1994enhancing}. 

At the end of the study, the facilitator conducted a follow-up interview, focusing on clarifying questions about issues the participant had encountered during the test. He also asked the participant
what was \textit{``liked about the tool''}, \textit{``wished would be different''}, and \textit{``under which circumstances s/he would use the tool''}.
% about what s/he \textit{``liked about the tool''}, what s/he \textit{``wished would be different''} and \textit{``under which circumstances s/he would use the tool''}. 
Finally, the participants were asked to complete a post-questionnaire. This consisted of multi-point Likert scales including the System Usability Scale (SUS) \cite{brooke1996sus} and scales covering satisfaction, expectation confirmation, continuation intention, and usefulness, adapted from ones~\cite{bhattacherjee2001understanding} used to assess continued use of information systems~\footnote{The complete questionnaire can be found at \url{https://git.io/Jv1HC}}.
The individual studies lasted between 14 and 35 minutes.

\begin{table}[t!]
	\caption{Questionnaire results represented as means across the different groups}
	\label{tab:questionnaire}
	\begin{tabular}{lccccc}
		 & \textbf{all} & \textbf{tier 1} & \textbf{tier 2} & \textbf{tier 3} & \textbf{tier 4 }\\
&  & \textbf{(P6, P7)} & \textbf{(P3, P8)} & \textbf{(P2, P4)} & \textbf{(P1, P5)}\\
		\midrule
        \textbf{SUS} & 73.13 & 71.25 & 81.25 & 67.50 & 72.50\\
        \textbf{Satisfaction} & 3.46 & 3.67 & 3.67 & 3.00 & 3.50\\
        \textbf{Expectation} & 3.69 & 3.67 & 3.50 & 3.50 & 4.00\\
        \textbf{Future intentions} & 2.61 & 2.33 & 2.00 & 2.00 & 3.67\\
        \textbf{Usefulness} & 3.00 & 2.63 & 3.13 & 2.75 & 3.50\\
		\bottomrule
	\end{tabular}
\end{table}

\mypar{Result analysis}
To analyze the collected data, the research team first created an affinity diagram~\cite{holtzblatt2004rapid} based on the video recordings, observations, and follow-up interviews, by creating a single paper note for each issue that was reported. The team then clustered the notes based on emerging themes, which resulted in 139 notes divided over 21 distinct clusters. The results of the questionnaire served as an additional qualitative data point during the analysis (\tablename~\ref{tab:questionnaire}).

\subsection{Findings}
\label{sec:evaluation:findings}
In general, our study revealed that the current interface was reasonably usable as evident by an average SUS score of 73.13 (\tablename~\ref{tab:questionnaire}), which can be considered to be a good results compared to the general average of 68 \cite{brooke1996sus}. However, it also pointed towards usability issues, which were addressed to create an improved version of the interface (the one shown in \figurename~\ref{fig:toolNew}). The main changes include the possibility to undo the addition to the model of the constraint derived from the last recording, editing the text that was recognized directly rather than having to re-record it, and adding data conditions  via speech recognition by selecting the respective record button (instead of specifying those conditions manually). 
%We also removed the previously included ``cancel'' button because it was often pressed by participants since they thought it would just stop the recording, whereas that button was supposed to discard the current recording.

\mypar{Common observations}
First, it should be noted that all participants used some means to \textbf{write sentences down} before saying them aloud. They either wrote them \textit{``on paper''} (P1), \textit{``typed them on [their] computer''} (P2), or noted down key parts they wanted to say (P7). Most participants also used somewhat \textbf{unnatural sentences} when entering constraints (\textit{``checking quantity must be executed before ordering wheels''}, P2). Some even explicitly tried to identify keywords that the algorithm might pick up for its translation (\textit{``is activity like a keyword?''}, P8). Almost all participants tried to formulate sentences with  activity names instead of proper natural language statements (\textit{``after check quantity order wheels is executed.''}, P7). Only P1 used natural sentences from the start. This issue can be related to
the artificial nature of the predefined scenario.
% the fact that the context was somehow ``artificial'' in the form of a  predefined scenario.
Although this was needed to make the evaluation homogeneous,  asking participants to describe a process of their choice could make the formulation more natural.

%belonging to domains the participants were familiar with, without any explicit suggestions, could make the input formulation more natural.

Trying to identify keywords and adapting the input statements 
for the tool 
%to allow the tool to recognize the vocal input 
might have \textbf{increased the complexity of entering constraints} in a way that might even be counterproductive when using speech recognition since our tool was designed to understand truly natural sentences.
These issues indicate that \textbf{individuals need to learn how to effectively use speech as a means of modeling constraints} regardless of their previous experience related to BPM or \Declare. Therefore, it would be desirable for users to go through a training phase to better understand how to interact with the tool and also appreciate the variety of vocal inputs the tool can deal with.

\mypar{Difference between participants with different backgrounds}
There were also noticeable differences between individuals from different backgrounds. For example, participants from tiers 1 and 2 (having low confidence with \Declare) \textbf{mainly relied on the visual process model} to assess whether the tool had understood them correctly, while participants from tiers 3 and 4 (more familiar with \Declare\ and analysis tools based on \Declare) \textbf{mainly relied on the list providing a semistructured representation} of the entered constraints (cf. bottom-right area of the screenshot in \figurename~\ref{fig:toolNew}). This was evident by how they tried to fix potential errors. Participants from tiers 1 and 2 initially tried to edit the visual process model (observation of P6, \textit{``The activity label should be editable''}, P2) -- which was not possible in the version of the tool we tested -- while participants from tiers 3 and 4 directly started editing the constraint list (observation of P1 and P5). Only P2 attempted both. %Given the different levels of confidence of the participants with \Declare, it appears reasonable that each group would mainly focus on the visualization that they are most familiar with.

%Taking the aforementioned findings together it appears as if creating spoken constraints that would be turned into formal constraints might have \textbf{increased the complexity of entering constraints} in particular for less experienced participants. Trying to identify keywords requires for users to formalize constraints in a way that might even be counterproductive when using speech input since our tool was created with the aim of understanding natural sentences. Moreover, it appears difficult for less experienced uses to assess the correctness of their entered constraints. This might be partly due to their potential lack of understanding \Declare. One approach to mitigate this issue might be to provide the option to mark constraints that a user cannot verify and subsequently ask an experienced user to assess the correctness.

%Trying to identify keywords and adapt the input statements to allow the tool to recognize the vocal input might have \textbf{increased the complexity of entering constraints} in particular for less experienced participants, since it requires users to formalize constraints in a way that might even be counterproductive when using speech input since our tool was created with the aim of understanding natural sentences. Moreover, it appears difficult for less experienced uses to assess the correctness of their entered constraints. This might be partly due to their potential lack of understanding \Declare. One approach to mitigate this issue might be to provide the option to mark constraints that a user cannot verify and subsequently ask an experienced user to assess the correctness.


Findings from our study also indicated that the way the \textbf{recognized text} was displayed (cf. text field below \textit{Voice input} in the top-left area of the screenshot in \figurename~\ref{fig:toolNew}) might not have been ideal. Participants checked the input correctness after they entered the first constraint, but did not continue to do so afterwards (observation of P3 and P6).
%only referred to it after they had entered the first constraint and then did not check it again for future constraints (observation of P3 and P6). 
This led to confusion, particularly among less experienced participants, who thought that they did not formulate a constraint correctly, while the issue was that their vocal input was not registered properly. 
One way to address this issue is to highlight the text field containing the recognized text directly after a new text has been recorded to indicate to the user that s/he should pay attention to the recording. 

These findings indicate that, even if using speech recognition users can somehow bypass the interaction with the graphical notation of \Declare\ constraints, it still appears difficult for less experienced users to assess the correctness of their entered constraints. One approach to mitigate this issue might be to provide the option to mark constraints that a user cannot verify and subsequently ask an experienced user to assess the correctness.

The aforementioned issues might also have led to \textbf{less experienced participants perceiving the tool as less useful and being less inclined to use the tool in the future} than experienced participants (cf. \tablename~\ref{tab:questionnaire}). Related to this finding, it thus appears counterintuitive that experienced participants (tiers 3 and 4) were slightly less satisfied with the tool than less experienced participants (tiers 1 and 2). This can potentially be explained by users \textit{``expect[ing] more types''} (P5), indicating that experienced participants might have had higher expectations regarding the functionality  than less experienced participants.

Finally it should be noted that participants from different backgrounds were \textit{``excited''} (P5) to use the tool and continued to record constraints after the actual evaluation was over (\textit{``let me see what happens when I [...]''}, P8). It thus appears reasonable to assume that addressing the identified shortcomings can positively influence the perception about the usability and usefulness of the tool.

\mypar{Potential usage scenarios}
The participants mentioned multiple scenarios during which they would consider speech as useful for entering constraints. Most of them stated that using speech would \textbf{improve their efficiency} because they could \textit{``quickly input stuff''} (P4) especially when reading \textit{``from a textual description''} (P1) with P5 pointing out that it would be particularly useful for \textit{``people that are not familiar with the editor''} (P5). On the other hand, P8 had a different opinion stating that \textit{``manually is quicker''} (P8). S/he did however acknowledge that speech might be useful \textit{``when I forget the constraint name''} (P8) thus indicating that \textbf{using natural language sentences} might be useful even when not using speech recognition. Moreover, participants also suggested that vocal input might be useful for \textit{``for designing''} (P3) which points towards the usefulness for specific activities at the early stages of modeling a process.
%
Finally P1 also mentioned that speech input would be useful for \textbf{mobile scenarios} such as \textit{``using a tablet''} (P1), because entering text in a scenario like that is usually much more time consuming than when using a mechanical keyboard.

\subsection{Threats to validity}
\label{sec:evaluation:limitations}
The goal of our study was to identify means of improving the tool, identify potential usage scenarios, and any issues for individuals with different backgrounds. It thus appeared reasonable to conduct an in-depth qualitative study with selected participants from a diverse range of backgrounds related to their knowledge and experience with Business Process Management in general and temporal properties and \Declare\ in particular. Conducting a study with a small sample of participants is common because research has shown that the number of additional insights gained deteriorates drastically per participant \cite{nielsen1993mathematical}. There are, however, some threats to validity associated with this particular study design. A first threat is related to the fact that we developed a specific tool and studied its use by specific people in a specific setting over a limited period of time. Despite carefully selecting participants and creating a setting that would be close to how we envision the tool be commonly used, it is not possible to generalize our findings beyond our study context since conducting the same study with different participants using a different setup might yield different results. In addition, the study was conducted by a team of researchers which poses a threat to validity since different researchers might potentially interpret findings differently. We attempted to mitigate this threat by ensuring that observations, interviews and the analysis of the obtained data was collaboratively conducted by two researchers. We also abstained from making causal claims instead providing a rich description of the behavior and reported perceptions of participants. 