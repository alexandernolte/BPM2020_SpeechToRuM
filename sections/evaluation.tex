\section{User Evaluation}
\label{sec:evaluation}

In order to improve the current implementation and assess the overall feasibility of the developed Speech2RuM approach, we conducted a qualitative case study. The threefold aim was to (1) identify means for improving the interface, (2) identify potential issues and demands of individuals with different backgrounds, and (3) identify potential usage scenarios.
In the following, we will first describe the study setup including the data we collected and our analysis procedure before discussing our findings.
\todo{Han: we have to decide on a consistent way to refer to the tool, e.g., interfact/tool/implementation}\todo{I tried to stick to "tool" but we can change it in any way you want.}

\subsection{Study setup}
\label{sec:evaluation:setup}

\mypar{Participants} For our study, we selected a total of eight participants with differing characteristics (referred to as P1 to P8 in \tablename~\ref{tab:questionnaire}).
In particular, we selected two participants that only had experience related to Business Process Management (tier 1), two participants with experience related to modeling temporal constraints (tier 2), two participants that had experience related using \Declare\ to model temporal constraints (tier 3), and two participants that had experience related to using \Declare\ and a variety of tools model and analyze temporal constraints (tier 4). We chose this differentiation to gain insight about the usability of the tool for individuals with different levels of expertise and to assess potential issues and demands by individuals with different backgrounds.

\mypar{Study setup}
The study was conducted via Skype by a team consisting of a \emph{facilitator} and an \emph{observer}, with the facilitator guiding the participant and the observer serving in a supporting role. We used the production and shipping process in a bicycle factory\footnote{The scenario can be found at \url{https://git.io/Jv1Hn}} as a background for the study because it can be expected that every participant will be somewhat familiar with such a process. We also created a help document\footnote{The help document can be found at \url{https://git.io/Jv1Hc}} that explains to the participants the different constraints that the tool could handle.

Prior to the study, the facilitator sent the participants a link to the scenario, the help document, and the tool itself. He suggested them to read the documents and install the tool prior to the study. At the beginning of the study, the facilitator introduced the participant to the study procedure and to the tool by showing how to enter a single constraint using vocal input. He also asked the participant to think aloud during the study. After any potential request for clarification, the participant shared her/his screen, the facilitator started the video recording, and began guiding the participant through the prepared scenario.
The participants were then asked to carry out multiple tasks starting with reading prepared sentences representing constraints related to the scenario
%to enter constraints before forming their own sentences
and then creating constraints without prior guidance. During this time, facilitator and observer noted down any issues the participant mentioned or any situations that appeared as he/she had issues using the tool based on the usability heuristics by Nielsen \cite{nielsen1994enhancing}. At the end of the study, the facilitator conducted a short follow-up interview focusing on clarifying questions about the observation or issues the participant had mentioned during the test. He also asked the participant about what he/she \textit{``liked about the tool''}, what he/she \textit{``wished would be different''} and \textit{``under which circumstances he/she would use the tool''}. After the study, the participants were asked to complete a short post-questionnaire. The questionnaires consisted of multi-point Likert scales including the System Usability Scale (SUS) \cite{brooke1996sus} and scales covering satisfaction, expectation confirmation, continuation intention and usefulness which were adapted from Bhattacherjee \cite{bhattacherjee2001understanding} to assess continued use of information systems\footnote{The complete questionnaire can be found at \url{https://git.io/Jv1HC}}.
The individual studies lasted between 14 and 35 minutes including interviews.  \todo{so short?}

\begin{table}[!ht]
	\caption{Questionnaire results represented as means across the different groups}
	\label{tab:questionnaire}
	\begin{tabularx}{\textwidth}{lccccc}
		 & \textbf{all} & \textbf{tier 1} & \textbf{tier 2} & \textbf{tier 3} & \textbf{tier 4 }\\
&  & \textbf{(P6, P7)} & \textbf{(P3, P8)} & \textbf{(P2, P4)} & \textbf{(P1, P5)}\\
		\midrule
        \textbf{SUS} & 73.13 & 71.25 & 81.25 & 67.50 & 72.50\\
        \textbf{Satisfaction} & 3.46 & 3.67 & 3.67 & 3.00 & 3.50\\
        \textbf{Expectation} & 3.69 & 3.67 & 3.50 & 3.50 & 4.00\\
        \textbf{Future intentions} & 2.61 & 2.33 & 2.00 & 2.00 & 3.67\\
        \textbf{Usefulness} & 3.00 & 2.63 & 3.13 & 2.75 & 3.50\\
		\bottomrule
	\end{tabularx}
\end{table}

\mypar{Result analysis}
To analyze the collected data, the research team first created an affinity diagram~\cite{holtzblatt2004rapid} based on the video recordings, observations, and follow-up interviews, by creating a single paper note for each issue that was reported. The team then clustered the notes based on emerging themes, which resulted in 139 notes divided over 21 distinct clusters. The results of the questionnaire served as an additional qualitative data point during the analysis (\tablename~\ref{tab:questionnaire}).

\subsection{Findings}
\label{sec:evaluation:findings}
In general, our study revealed that the current interface was reasonably usable as evident by an average SUS score of 73.13 (\tablename~\ref{tab:questionnaire}), which can be considered to be a good results compared to the general average of 68 \cite{brooke1996sus}. Our study, however, also pointed towards multiple usability issues, which were addressed to create an improved version of the interface (\figurename~\ref{fig:tool}). %enter reference to new interface here
These include the possibility to simply undo the last recording, editing the text that was recognized directly rather than recording again and adding data conditions (activation, correlation, time) via speech recognition by selecting the respective record button. We also removed the previously included ``cancel'' button because it was often pressed by participants since they thought it would just stop the recording, whereas that button was supposed to discard the current recording.

\subsubsection{Difference between participants with different backgrounds}
First it should be noted that all participants used some means to \textbf{write sentences down} before saying them out loud. They either wrote them \textit{``on paper''} (P1), \textit{``typed them on [their] computer''} (P2) or they noted down key parts that they wanted to say (observation of P7). Most participants also used somewhat \textbf{unnatural sentences} when entering constraints (\textit{``ordering wheels must be executed before''}, P2). Some even explicitly tried to identify keywords (\textit{``Is activity like a keyword?''}, P8) that the algorithm would pick up and translate into the desired constraint (\textit{``Oh it didn't get this one. How about this? Activity order wheels must be executed before''}, P7). Only P1 used natural sentences from the start. This finding indicates that individuals need to learn about how to effectively use speech as a means of modeling constraints regardless of their previous experience related to Business Process Management or \Declare. This can be related to the fact that the participant were provided with a predefined scenario and the context was somehow ``artificial''. This was needed to somehow make the evaluation homogeneous from the point of view of the scenario. However, coming up with constraints belonging to domains the participants were familiar with, without any explicit suggestions could make the formulation of the vocal inputs more natural.

There were, however, also noticeable differences between individuals from different background starting with participants from tiers 1 and 2 \textbf{mainly relying on the visual process model} to assess whether the tool had understood them correctly while participants from tiers 3 and 4 \textbf{mainly relied on the textual representations} of the entered constraints. This was mainly evident by how they tried to fix potential errors. Participants from tiers 1 and 2 initially tried to edit the visual process model (observation of P6, \textit{``I want to de-merge the visualization''}, P3) -- which was not possible in the version of the tool we tested -- while participants from tiers 3 and 4 directly started to edit the textual constraints (observation of P1 and P5). Only P2 attempted both. This difference might be related to participants from tiers 3 and 4 already being familiar with \Declare\ while participants from tiers 1 and 2 were more familiar with visual process representations, e.g., in the form of BPMN models. It thus appears reasonable that each group would mainly focus on the visualization that they are most familiar with.

Findings from our study also indicated that the way the \textbf{recognized text} was displayed (figure %enter reference to old interface here
) might not have been ideal since participants only referred to it after they had entered the first constraint and then did not check it again for future constraints (observation of P3 and P6). This led to confusion in particular among less experienced participants who thought that they did not formulate the constraint correctly while the issue simply was that their verbalized sentence was not correctly recognized.

The previously discussed issues might also have led to \textbf{less experienced participants perceiving the tool as less useful and being less inclined to use the tool in the future} than experienced participants (\tablename~\ref{tab:questionnaire}). Related to this finding, it thus does appear counterintuitive that experienced participants (tiers 3 and 4) were less satisfied with the tool than less experienced participants (tiers 1 and 2). This finding can potentially be explained by the small sample size, but also by P5 expressing that he/she would have \textit{``expected more constraint types''} (P5) indicating that experienced participants might have had more expectations related to the functionality of the tool than less experienced participants.

Taking the aforementioned findings together it appears as if creating spoken constraints that would be turned into formal constraints might have \textbf{increased the complexity of entering constraints} in particular for less experienced participants. Writing down sentences before speaking them out and trying to identify keywords requires for users to formalize constraints in a way that might even be counterproductive when using speech input since our tool was created with the aim of understanding natural sentences. Moreover, it appears difficult for less experienced uses to assess the correctness of their entered constraints due to their potential lack of understanding \Declare. It thus appears reasonable to consider saving the recorded text in addition to the created constraints, which would allow experienced users to assess the entered constraints post-hoc.

\subsubsection{Potential usage scenarios}
The participants mentioned multiple scenarios during which they would consider speech as useful for entering constraints. Most of them stated that using speech would \textbf{improve their efficiency} because they could \textit{``quickly input stuff''} (P4) with P5 pointing out that it would be particularly useful for \textit{``people that are not familiar with the editor''} (P5). P8 however had a different opinion stating that \textit{``manually is quicker''} (P8). He/she did however acknowledge that speech might be useful \textit{``when I forget the constraint name''} (P8). He/she thus indicated that \textbf{using natural language sentences} might be useful even when not using speech. Related to this, participants suggested that vocal input might be useful \textit{``for designing''} (P3) or when reading \textit{``from a textual description''} (P1). This finding also points towards the potential necessity for \textbf{post-processing} the entered constraints.

Finally P1 also mentioned that speech input would be useful for \textbf{mobile scenarios} such as \textit{``using a tablet''} (P1), because entering text in a scenario like that is usually much more time consuming than when using a mechanical keyboard. 